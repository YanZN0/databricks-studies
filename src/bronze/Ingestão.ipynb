{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052e52d3-6ccb-453e-8687-1adb2c491adf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import delta\n",
    "\n",
    "def table_exists(catalog, database, table):\n",
    "    \n",
    "    count = (spark.sql(f\"SHOW TABLES FROM {catalog}.{database}\")\n",
    "            .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "            .count())\n",
    "    return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c4ccec-6dee-4a66-98d0-abfa528d164c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SETUP"
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'bronze'\n",
    "schema = 'olist_ecommerce'\n",
    "tablename = dbutils.widgets.get('tablename')\n",
    "id_field = dbutils.widgets.get('id_field')\n",
    "timestamp_field = dbutils.widgets.get('timestamp_field')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8692892d-3303-44a8-8fa0-a333187f2dbf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão Full Load Camada Bronze"
    }
   },
   "outputs": [],
   "source": [
    "# Esta camada de código tem o objetivo de fazer a ingestão de um arquivo Full Load na pasta de Bronze. \n",
    "# O arquivo Full Load é uma Tabela em formato CSV, que contém dados mas que precisam ser atualizados através de arquivos CDC (Change Data Capture).\n",
    "\n",
    "# Nesta parte do código, precisamos primeiro ler o arquivo CSV com o Spark que logo após a leitura se transforma em um Dataframe. \n",
    "# No código podemos notar .option(\"header\", \"true\"), utilizei esse comando pois com ele conseguimos nos comunicar com o Spark e dizer que o arquivo CSV contém cabeçalho, ou seja, nome de colunas presentes na primeira.\n",
    "\n",
    "if not table_exists(catalog, schema, tablename):\n",
    "\n",
    "  print('Tabela não existente. Criando...')\n",
    "\n",
    "\n",
    "  df_full_table = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"/Volumes/raw/olist_ecommerce/full_load/{tablename}/\")\n",
    "\n",
    "    # Logo após a leitura preciso salvar o Dataframe no Schema de Bronze.\n",
    "\n",
    "  (df_full_table.coalesce(1)    # Com coalesce(1), garantimos que o Spark nos entregue apenas 1 arquivo salvo.\n",
    "    .write\n",
    "    .format(\"delta\")    # Formato de salvamento do arquivo.\n",
    "    .mode(\"overwrite\")   # Modo de salvamento, aqui se caso ja contesse dados no Schema o \"overwrite\" iria subscreve-los.\n",
    "    .saveAsTable(f\"{catalog}.{schema}.{tablename}\"))   # Aqui é onde dizemos para o Spark o caminho de salvamento, e especicamos que no Schema o arquivo será salvo como Tabela.\n",
    "  \n",
    "else:\n",
    "  print('Tabela já existente :/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd06db66-d272-4722-9c87-b73f2626cf4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze.olist_ecommerce.orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbdbf45-fa1e-48a7-b0e5-d5b6e78d7e34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importação de Arquivos CDC"
    }
   },
   "outputs": [],
   "source": [
    "# Nesta camada de código preciso da importação de arquivos CDC. Onde no futuro esses arquivos que precisam ser atualizados serão a principal fonte de dados para a atualização da Tabela presente no Schema Bronze. \n",
    "\n",
    "# Importando e fazendo a leitura de dados com o Spark.\n",
    "(spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(f\"/Volumes/raw/olist_ecommerce/cdc/{tablename}/\")\n",
    "    .createOrReplaceTempView(f\"view_{tablename}\"))  # Essa linha de código nos permite transformar o Dataframe que foi lido pelo Spark em uma Tabela Temporária, nos permitindo fazer consultas SQL.\n",
    "\n",
    "query = f''' \n",
    "SELECT *\n",
    "FROM view_{tablename}\n",
    "QUALIFY ROW_NUMBER() OVER(PARTITION BY {id_field} ORDER BY {timestamp_field} DESC) = 1\n",
    "'''     # Nesta parte do código estamos fazendo uma consulta SQL, onde tenho o objetivo de fazer um filtro com que apareça os dados mais atualizados de cada cliente presente no arquivo CDC.\n",
    "\n",
    "df_cdc_table = spark.sql(query)    # Transformando o resultado da consulta SQL em um Dataframe.\n",
    "df_cdc_table.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3f36d58-85ec-4f30-a189-97ad58488f56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Mesclando Dados Atualizados na Tabela"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui é onde iremos fazer um Merge para atualizar a Tabela presente no Schema Bronze. A Tabela só contém dados Full Load desatualizados, e com os dados obtidos acima através de arquivos CDC, iremos fazer um Merge para a atualização da Tabela.\n",
    "\n",
    "bronze = delta.DeltaTable.forName(spark, f\"{catalog}.{schema}.{tablename}\") # Aqui conseguimos fazer com que, conversamos com nossa tabela através de uma variável.\n",
    "\n",
    "(bronze.alias(\"b\")\n",
    "     .merge(df_cdc_table.alias(\"c\"), f\"b.{id_field} = c.{id_field}\")    # Fazendo um JOIN entre as tabelas.\n",
    "     .whenMatchedDelete(condition = \"c.OP = 'D'\")      # Condição para deletar um dado presente na Tabela Bronze.\n",
    "     .whenMatchedUpdateAll(condition = \"c.OP = 'U'\")   # Condição para atualizar um dado presente na Tabela Bronze\n",
    "     .whenNotMatchedInsertAll(condition = \"c.OP = 'I' OR c.OP = 'U'\") # Condição para inserir um dado novo na Tabela Bronze.\n",
    "     .execute()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc90a5b-4d42-4c60-9859-38272f5c4a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze.olist_ecommerce.orders\n",
    "where order_id = '76d3da933ea02dff5b27a64415350c37'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6757400572125112,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestão",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

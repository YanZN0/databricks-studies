{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052e52d3-6ccb-453e-8687-1adb2c491adf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import delta\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../lib/\")   # Importando funções da Lib.\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c4ccec-6dee-4a66-98d0-abfa528d164c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SETUP"
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'bronze'\n",
    "schema = 'olist_ecommerce'\n",
    "tablename = dbutils.widgets.get('tablename')\n",
    "id_field = dbutils.widgets.get('id_field')\n",
    "timestamp_field = dbutils.widgets.get('timestamp_field')\n",
    "df_schema = utils.import_schema(tablename)  # Função que importa o schema do JSON e relaciona com a tabela \n",
    "# a depender do nome. Para conseguir os Schemas, precisei cria-los a mão no código, tive essa abordagem pois arquivos CSV # tudo é considerado texto e string, ou seja, não existe valores bool, float ou int.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8692892d-3303-44a8-8fa0-a333187f2dbf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão Full Load Camada Bronze"
    }
   },
   "outputs": [],
   "source": [
    "# Esta camada de código tem o objetivo de fazer a ingestão de um arquivo Full Load na pasta de Bronze. \n",
    "# O arquivo Full Load é uma Tabela em formato CSV, que contém dados mas que precisam ser atualizados através de arquivos\n",
    "# CDC (Change Data Capture).\n",
    "\n",
    "# Nesta parte do código, precisamos primeiro ler o arquivo CSV com o Spark que logo após a leitura se transforma em um\n",
    "# Dataframe. \n",
    "# No código podemos notar .option(\"header\", \"true\"), utilizei esse comando pois com ele conseguimos nos comunicar\n",
    "# com o Spark e dizer que o arquivo CSV contém cabeçalho, ou seja, nome de colunas presentes na primeira.\n",
    "\n",
    "if not utils.table_exists(spark, catalog, schema, tablename): # Utilizando um IF, Se a tabela existir a função retorna 1 \n",
    "# e não faz nada. Mas se a tabela não existir faz a ingestão da full load na camada de Bronze.\n",
    "\n",
    "  print('Tabela não existente. Criando...')\n",
    "\n",
    "\n",
    "  df_full_table = spark.read.format(\"csv\").option(\"header\", \"true\").schema(df_schema).load(f\"/Volumes/raw/olist_ecommerce/full_load/{tablename}/\")\n",
    "\n",
    "\n",
    "    # Logo após a leitura preciso salvar o Dataframe no Schema de Bronze.\n",
    "\n",
    "  (df_full_table.coalesce(1)    # Com coalesce(1), garantimos que o Spark nos entregue apenas 1 arquivo salvo.\n",
    "    .write\n",
    "    .format(\"delta\")    # Formato de salvamento do arquivo.\n",
    "    .mode(\"overwrite\")  # Modo de salvamento, aqui se caso ja contesse dados no Schema o \"overwrite\" iria subscreve-los.\n",
    "    .saveAsTable(f\"{catalog}.{schema}.{tablename}\"))   # Aqui é onde dizemos para o Spark o caminho de salvamento, e especicamos que no Schema o arquivo será salvo como Tabela.\n",
    "  \n",
    "else:\n",
    "  print('Tabela já existente :/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7209cb5-b1d1-48b9-858d-581ba9cf6aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze.olist_ecommerce.customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8cd67f-e7f3-4c9c-ab7c-f3234a367a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze = delta.DeltaTable.forName(spark, f\"{catalog}.{schema}.{tablename}\") # Aqui conseguimos fazer com que, conversamos com nossa tabela através de uma variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbdbf45-fa1e-48a7-b0e5-d5b6e78d7e34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importação de Arquivos CDC"
    }
   },
   "outputs": [],
   "source": [
    "# Nesta camada de código preciso da importação de arquivos CDC. Onde no futuro esses arquivos que precisam ser atualizados\n",
    "# serão a principal fonte de dados para a atualização da Tabela presente no Schema Bronze.\n",
    "\n",
    "def upsert(df, deltatable):\n",
    "\n",
    "    df.drop(\"modifed_at\")\n",
    "    df.createOrReplaceGlobalTempView(f\"view_{tablename}\")   # Global View pode ser acessada de qualquer sessão do Spark, pois é uma View Global.\n",
    "\n",
    "    query = f''' \n",
    "        SELECT *\n",
    "        FROM global_temp.view_{tablename}\n",
    "        QUALIFY ROW_NUMBER() OVER(PARTITION BY {id_field} ORDER BY {timestamp_field} DESC) = 1\n",
    "    '''     # Nesta parte do código estamos fazendo uma consulta SQL, onde tenho o objetivo de fazer um filtro \n",
    "            # com que apareça os dados mais atualizados de cada cliente presente no arquivo CDC.\n",
    "\n",
    "    df_cdc_table = spark.sql(query)    # Transformando o resultado da consulta SQL em um Dataframe.\n",
    "    \n",
    "# Merge para atualizar a tabela com dados recentes dos arquivos CDC.\n",
    "    (deltatable.alias(\"b\")\n",
    "           .merge(df_cdc_table.alias(\"c\"), f\"b.{id_field} = c.{id_field}\")    # Fazendo um JOIN entre as tabelas.\n",
    "           .whenMatchedDelete(condition = \"c.OP = 'D'\")      # Condição para deletar um dado presente na Tabela Bronze.\n",
    "           .whenMatchedUpdateAll(condition = \"c.OP = 'U'\")   # Condição para atualizar um dado presente na Tabela Bronze\n",
    "           .whenNotMatchedInsertAll(condition = \"c.OP = 'I' OR c.OP = 'U'\") # Condição para inserir um dado novo \n",
    "                                                                            # na Tabela Bronze.\n",
    "           .execute()\n",
    "\n",
    ") \n",
    "\n",
    "\n",
    "df_stream = (spark.readStream  # Importando e fazendo a leitura de dados CDC com o Spark e Streaming.\n",
    "                  .format(\"cloudFiles\")\n",
    "                  .option(\"cloudFiles.format\", \"csv\")\n",
    "                  .schema(df_schema)\n",
    "                  .load(f\"/Volumes/raw/olist_ecommerce/cdc/{tablename}/\")\n",
    "    )\n",
    "\n",
    "stream = (df_stream.writeStream\n",
    "            .option(\"checkpointLocation\", f\"/Volumes/raw/olist_ecommerce/cdc/{tablename}_checkpoint/\")\n",
    "            # Pasta que controla o último progresso feito pelo stream.\n",
    "            .foreachBatch(lambda df, bathID: upsert(df, bronze)) \n",
    "# Para cada lote de dados que vier pelo stream, ele aplica uma função de upsert que recebe \"df\"(lote de dados), \n",
    "# faz a mesclagem com Merge e salva na base bronze (deltatable).\n",
    "            .trigger(availableNow=True)\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed162a9d-338e-4451-900a-e470095df44a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = stream.start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6206266361093360,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestão",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

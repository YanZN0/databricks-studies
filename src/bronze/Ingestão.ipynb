{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052e52d3-6ccb-453e-8687-1adb2c491adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8692892d-3303-44a8-8fa0-a333187f2dbf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão Full Load Camada Bronze"
    }
   },
   "outputs": [],
   "source": [
    "# Esta camada de código tem o objetivo de fazer a ingestão de um arquivo Full Load na pasta de Bronze. O arquivo Full Load é uma Tabela em formato CSV, que contém dados mas que precisam ser atualizados através de arquivos CDC (Change Data Capture).\n",
    "\n",
    "# Nesta parte do código, precisamos primeiro ler o arquivo CSV com o Spark que logo após a leitura se transforma em um Dataframe. No código podemos notar .option(\"header\", \"true\"), utilizei esse comando pois com ele conseguimos nos comunicar com o Spark e dizer que o arquivo CSV contém cabeçalho, ou seja, nome de colunas presentes na primeira.\n",
    "df_full_table = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Volumes/raw/olist_ecommerce/full_load/customers/\")\n",
    "\n",
    "# Logo após a leitura preciso salvar o Dataframe no Schema de Bronze.\n",
    "(df_full_table.coalesce(1)    # Com coalesce(1), garantimos que o Spark nos entregue apenas 1 arquivo salvo.\n",
    "  .write\n",
    "  .format(\"delta\")    # Formato de salvamento do arquivo.\n",
    "  .mode(\"overwrite\")    # Modo de salvamento, aqui se caso ja contesse dados no Schema o \"overwrite\" iria subscreve-los.\n",
    "  .saveAsTable(\"bronze.olist_ecommerce.customers\"))   # Aqui é onde dizemos para o Spark o caminho de salvamento, e especicamos que no Schema o arquivo será salvo como Tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ece5923-3637-45ff-b76f-951a5cb0cd10",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verificando Existência da Tabela"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze.olist_ecommerce.customers\n",
    "LIMIT 100\n",
    "\n",
    "-- Apenas verifico aqui se o arquivo foi salvo como Tabela com SQL na pasta de Bronze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbdbf45-fa1e-48a7-b0e5-d5b6e78d7e34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importação de Arquivos CDC"
    }
   },
   "outputs": [],
   "source": [
    "# Nesta camada de código preciso da importação de arquivos CDC. Onde no futuro esses arquivos que precisam ser atualizados serão a principal fonte de dados para a atualização da Tabela presente no Schema Bronze. \n",
    "\n",
    "# Importando e fazendo a leitura de dados com o Spark.\n",
    "(spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"/Volumes/raw/olist_ecommerce/cdc/customers/\")\n",
    "    .createOrReplaceTempView(\"customers\"))  # Essa linha de código nos permite transformar o Dataframe que foi lido pelo Spark em uma Tabela Temporária, nos permitindo fazer consultas SQL.\n",
    "\n",
    "query = \"\"\" \n",
    "SELECT *\n",
    "FROM customers\n",
    "QUALIFY ROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY modifed_at DESC) = 1\n",
    "\"\"\"     # Nesta parte do código estamos fazendo uma consulta SQL, onde tenho o objetivo de fazer um filtro com que apareça os dados mais atualizados de cada cliente presente no arquivo CDC.\n",
    "\n",
    "df_cdc_table = spark.sql(query)     # Transformando o resultado da consulta SQL em um Dataframe.\n",
    "df_cdc_table.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3f36d58-85ec-4f30-a189-97ad58488f56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Mesclando Dados Atualizados na Tabela"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui é onde iremos fazer um Merge para atualizar a Tabela presente no Schema Bronze. A Tabela só contém dados Full Load desatualizados, e com os dados obtidos acima através de arquivos CDC, iremos fazer um Merge para a atualização da Tabela.\n",
    "\n",
    "bronze = delta.DeltaTable.forName(spark, \"bronze.olist_ecommerce.customers\") # Aqui conseguimos fazer com que, conversamos com nossa tabela através de uma variável.\n",
    "\n",
    "(bronze.alias(\"b\")\n",
    "     .merge(df_cdc_table.alias(\"c\"), \"b.customer_id = c.customer_id\")    # Fazendo um JOIN entre as tabelas.\n",
    "     .whenMatchedDelete(condition = \"c.OP = 'D'\")      # Condição para deletar um dado presente na Tabela Bronze.\n",
    "     .whenMatchedUpdateAll(condition = \"c.OP = 'U'\")   # Condição para atualizar um dado presente na Tabela Bronze\n",
    "     .whenNotMatchedInsertAll(condition = \"c.OP = 'I' OR c.OP = 'U'\") # Condição para inserir um dado novo na Tabela Bronze.\n",
    "     .execute()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae4373f-cc53-4369-8c8b-48759fbcff52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bronze.olist_ecommerce.customers\n",
    "WHERE customer_id = '00a16acd591b4bb112f2e94c7d2f9936'\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5316721443600052,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestão",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
